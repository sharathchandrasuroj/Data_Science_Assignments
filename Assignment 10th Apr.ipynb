{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a9c834",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d183a",
   "metadata": {},
   "source": [
    "#We can use Bayes' theorem to calculate the probability that an employee is a smoker given that he/she uses the health insurance plan:\n",
    "\n",
    "P(smoker | uses insurance) = P(uses insurance | smoker) * P(smoker) / P(uses insurance)\n",
    "\n",
    "We are given that 70% of the employees use the company's health insurance plan, so P(uses insurance) = 0.7. We are also given that 40% of the employees who use the plan are smokers, so P(uses insurance | smoker) = 0.4. Finally, we know that the overall percentage of smokers among all employees is not provided.\n",
    "\n",
    "For the sake of this example, let's assume it is 20%, i.e. P(smoker) = 0.2.\n",
    "\n",
    "Now we can substitute these values into the formula:\n",
    "\n",
    "P(smoker | uses insurance) = 0.4 * 0.2 / 0.7 = 0.114\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.114 or approximately 11.4%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806e4e8",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5251e",
   "metadata": {},
   "source": [
    "#Bernoulli Naive Bayes and Multinomial Naive Bayes are both variations of Naive Bayes algorithm that are commonly used for text classification.\n",
    "\n",
    "The key difference between them lies in the way they represent the input features.\n",
    "\n",
    "Bernoulli Naive Bayes assumes that the input features are binary (i.e., either present or absent). \n",
    "\n",
    "For example, in text classification, the presence or absence of each word in a document can be considered a binary feature. Therefore, in Bernoulli Naive Bayes, each feature is represented by a binary value indicating whether it is present or absent. The probability of a feature given a class is calculated as the number of times the feature appears in documents of that class divided by the total number of documents of that class.\n",
    "\n",
    "#On the other hand, Multinomial Naive Bayes assumes that the input features represent count data (i.e., how many times a particular feature occurs).\n",
    "\n",
    "In text classification, this means that each feature is represented by the count of the number of times it appears in a document. The probability \n",
    "of a feature given a class is calculated as the sum of the counts of that feature in all documents of that class, divided by the total number of words in those documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207342a9",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6787b",
   "metadata": {},
   "source": [
    "#In Bernoulli Naive Bayes, missing values are typically handled by ignoring them during the model training phase. When the algorithm encounters a \n",
    "missing value during model training, it simply skips that feature and continues with the next feature. This is because Bernoulli Naive Bayes assumes that the input features are binary (i.e., either present or absent), and if a feature is missing, it is not considered present or absent, and therefore cannot be used for classification.\n",
    "\n",
    "During model prediction, if a test instance contains a missing value, the algorithm treats that feature as absent (i.e., it assumes that the feature\n",
    "is not present in the instance). This is based on the assumption that if a feature is not present in a document, it is equivalent to the feature \n",
    "being missing or unknown.\n",
    "\n",
    "In some cases, it may be possible to use imputation techniques to fill in missing values before training the Bernoulli Naive Bayes model.\n",
    "\n",
    "For example, one could use a simple imputation method such as replacing missing values with the mode (most common value) of that feature \n",
    "across the training data. However, this approach should be used with caution, as it may introduce bias into the model if the imputed values are\n",
    "not representative of the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc9339",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa402d",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. \n",
    "\n",
    "In multi-class classification, there are more than two classes, \n",
    "and the goal is to predict the class of a new instance based on its input features.\n",
    "\n",
    "In Gaussian Naive Bayes, the algorithm assumes that the input features are continuous and follows a Gaussian (normal) distribution. To use Gaussian Naive Bayes for multi-class classification, the algorithm calculates the probability of each class for a new instance, based on its input features, using Bayes' theorem. The class with the highest probability is then assigned as the predicted class.\n",
    "\n",
    "The algorithm calculates the probability of a new instance belonging to a particular class using the class-specific mean and variance of each feature. In the case of multi-class classification, the mean and variance of each feature are calculated for each class separately.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be used for multi-class classification by calculating the probability of each class for a new instance and\n",
    "selecting the class with the highest probability as the predicted class. The algorithm uses the class-specific mean and variance of each feature to calculate these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40804920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601ec19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825b4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70042888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b91c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fdd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e53ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ee087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287684e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ba6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60005667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0954280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf95ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53e8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc14243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
